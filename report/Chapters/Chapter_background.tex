% Chapter X

\chapter{Background} % Chapter title

\label{ch:background} % For referencing the chapter elsewhere, use \autoref{ch:name} 

In the following sections several techniques are described that are used in this research. This can give the reader adequate background information to fully understand the implementation.

%----------------------------------------------------------------------------------------

\section{JPEG Quantization Tables}\label{sec:dqt}
JPEG quantization tables are created during the JPEG compression phase. In this section a description is given of the JPEG compression method.
%and the creation of the JPEG quantization tables.

The storing of a raw image format is often undesirable since this requires much storage space. In order to reduce the storage space for an image, compression methods are used to create an appropriate trade-off between file size and image quality. JPEG is a commonly used method for lossy compression of digital images. This compression technique is based on the discrete cosine transform (DCT) and is lossy because the original image information is lost and cannot be restored, possibly affecting image quality. 

The JPEG compression is composed out of several steps which are depicted in \autoref{fig:jpeg}. First, a color space conversion is made from the \textit{RGB} domain to the $YC_bC_r$  domain. The $YC_bC_r$ domain uses luminance, chrominance blue and chrominance red. The luminance describes the brightness of the pixel while the chrominance carries information about its hue. This color space conversion is chosen because people are significantly more sensitive for changes in luminance than chrominance and as a result the chrominance channels can be down sampled easily with almost no visual effect. 

In the second step the image is split into blocks of 8 $\times$ 8 pixels. For each block, each of the Y, $C_b$ and $C_r$ data undergoes the DCT. The DCT transforms a signal or image from the spatial domain to the frequency domain. Thirdly, the amplitudes of the frequency components are quantized. Because human vision is more sensitive to variations over large areas than to strength of high-frequency brightness variations,  the magnitudes of the high-frequency components are stored with a lower accuracy than the low-frequency components. Each component in the frequency domain is divided by a constant for that component, and then rounded to the nearest integer. These constants are stored in quantization tables. Seperate quantization tables for both the luminance as the chrominance domain are used, where chrominance blue and chrominance red are often combined to one table. The elements in the quantization table thus control the compression ratio.

In the last step of JPEG compression, entropy encoding is used. The image components are arranged in a "zigzag" order after which the compression method employs a run-length encoding (RLE) algorithm. This algorithm stores sequences of data as a single data value and count, rather than as the original run. It then inserts length coding zeros and uses Huffman coding.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{Images/JPEG_quantization2.jpg}
\captionsetup{justification=centering}

\caption{JPEG compression steps\\~\\{\small Source: Fig. 1. DCT-Based Encoder Processing Steps. From: Wallace, G. K. (1992). \textit{The JPEG still picture compression standard.} Consumer Electronics, IEEE Transactions on, 38(1), xviii-xxxiv.\cite{wallace1992jpeg}}}
\label{fig:jpeg}
%\end{center}

\end{figure}

The JPEG image is saved with a JPEG File Interchange Format (JFIF) which contains several headers. These headers consist of markers and their associated information and are used for compatibility. For example, JPEG does not define the color space that is to be used for the image. JFIF has a marker in which the color space in use can be defined. The JPEG quantization tables are also stored in these headers.



\section{Decision Tree Learning}\label{sec:dt}

Decision tree learning is a supervised machine learning algorithm. It aims to construct the best decision tree model from class-labelled training. It is a predictive modelling approach as its goal is to create models to predict a value of a target based on input variables which are the target attributes.

A decision tree has a flow-chart-like structure where internal nodes denote a test on attributes and the leaf nodes hold a class label. Each branch corresponds to an attribute value. The decision tree holds a mapping of observations in attributes of a target to a class. The decision tree learning algorithm aims to create the best `splits' (i.e. the attribute tests) by finding patterns in the set of attributes. These splits are learned by recursive partitioning: the algorithm splits the data set into subsets with specific attribute values (i.e. patterns) and repeats this for the subsets until the subset is correctly partitioned to belong to the same target value, or when splitting no longer adds value to predictions.

The decision tree learning algorithm has many advantages. The decision tree is easy to interpret since it is comprised out of rules of boolean logic. In contrast to other machine learning algorithms, it requires little data preparation such as data normalization. For these reasons, a decision tree model can be implemented in the use of other search systems, for example in the use of databases queries. The decision tree learning algorithm is also able to handle both numerical and categorical data. This is useful since the quantization tables used in this research comprise of numerical values. In addition, the cost for predicting data is logarithmic in the number of data points used to train the tree. 

However, the decision tree learning algorithm also has some disadvantages. The decision tree learning algorithm can create over-complex trees as it is a greedy algorithm: it creates locally optimal solutions at the splits of the decision tree that approximate a global optimal solution. This can result into overfitting of the data as it does not generalize well from the training data. Decision trees can therefore be somewhat unstable as small variations in the data can result into different decision trees. 

In this research the decision tree learning algorithm is chosen mainly because it is easy to interpret and little data preparation is needed. Another reason is that its decision tree can be implemented in other search systems.




\iffalse
In order to optimize search through JPEG quantization tables the search space needs to be decreased. This reduction in search space can be performed by creating a decision tree model. This model maps observations about an item (specific features of the quantization table) to conclusions about the item's target value (camera model). Decision tree learning is used, which is the construction of a decision tree from class-labelled training tuples, to identify important parameters and their position in the decision tree model. The matching with the use of decision tree model parameters and the matching between full JPEG quantization tables are both benchmarked for time to see whether the search time is accelerated.

The following steps are taken:
\begin{enumerate}
\item Gather dataset of JPEG quantization tables. Dataset of pictures and their JPEG quantization table and for JPEG quantization for camera models are needed.
\item Create numerous possible parameters to identify these tables. Rewrite JPEG quantization table as collection of these parameter values.
\item Create training and test set for decision tree learning.
\item Perform decision tree learning to create decision tree model
\item Perform benchmarks: matching with the decision tree model parameters and matching full JPEG quantization tables
\end{enumerate}
\fi
