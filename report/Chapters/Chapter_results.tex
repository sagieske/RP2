% Chapter X

\chapter{Results} % Chapter title

\label{ch:results} % For referencing the chapter elsewhere, use \autoref{ch:name}

In this chapter the results of this research will be discussed. %\autoref{sec:res_activity} shows the results of the activity classification. In \autoref{sec:res_extraction} the results of the two different activity extraction methods are described and finally, in \autoref{sec:res_ranking}, the results for ranking popular activities are shown.

\section{Extraction of JPEG quantization tables}
In total there are 1,016 unique sets of JPEG quantization tables retrieved from the images. In \autoref{tab:dqt_count} the number of unique JPEG quantization tables per camera model are shown. As can be derived, there are JPEG quantization tables that are found for more than one camera model. There are 398 quantization tables found in images with different camera models. 

As the chrominance color space can be divided into chrominance-red and chrominance-blue, it could have occurred that 3 JPEG quantization tables are retrieved for an image. However, only sets of 2 JPEG quantization tables are found in the dataset.

\section{Feature Selection}
The two JPEG quantization tables are converted to a set of features and the extra statistical features (as described in \autoref{sec:featselect}) are added. Each JPEG quantization table contains 64 values and 105 extra statistical features per table are added, which gives a total of 338 attributes per image. The extra features did not have impact on the scores for the evaluation. Therefore, these extra statistical features are omitted from the feature set.

After the feature selection procedure the identifiable parameters for the decision tree learning algorithm are reduced to 50 parameters. In table \autoref{tab:important} the importance of every attribute is depicted. The parameters do not show a clear correlation with the tables. The total importance of the luminance JPEG quantization table is 0.56 and total importance of the chrominance JPEG quantization table: is 0.44. 
This shows that both JPEG quantization tables have a comparable importance.


\section{Decision Tree Learning}
The decision tree learning algorithm has created a decision tree of 603 nodes with a depth of 26 nodes. The average F$_2$-score is 89\% for the prediction of the camera make and 80\% for the prediction of the camera model. The exact F$_2$-scores for every camera make and model are described in \autoref{tab:fscore_make} and \autoref{tab:fscore_model}, respectively.

The decision tree model gains high scores for the prediction of the camera make. The average F$_2$-scores is for many camera makes higher than 80\%. There are a few camera makes which have lower F$_2$-score, such as Motorola. This result is unexpected, because the dataset contains 4060 images made by Motorola cameras and only 6 unique sets of JPEG quantization tables are found. This result can be explained by the occurrence of these sets in images from other camera makes

%In the following subsections the results of the decision tree evaluation is discussed as well as the comparison against the hash database models.



%\subsection{Evaluation}

\section{Comparison against hash database}
The decision tree model is compared to the two hash database models that are explained in \autoref{sec:impl_hash}. They are compared for the prediction of the camera make as well as the prediction of the camera model. An overview of the scores is given in \autoref{tab:fscore_make} and \autoref{tab:fscore_model}.

The decision tree model has the highest F$_2$-score for the prediction of the camera make. For the prediction of the camera model it scores 3\% lower on the F$_2$-score than the 1$\rightarrow N$ hash database model. Overall, the decision tree model scores better than the 1$\rightarrow$1 hash database model and comparable to the 1$\rightarrow N$ hash database model.

The 1$\rightarrow N$ hash database model scores high on recall for both predictions. This result is explained by the fact that the hash database model stores all possible camera make and models for each unique set of JPEG quantization tables. It returns all possibilities and will receive a high recall as a result of the probability that the correct camera make/model is in the returned set is very high. However, it receives low precision rates for the predictions of the camera make as well for the prediction of the camera model. This is also a result of the method returning all possibilities as many false positives are returned.

The 1$\rightarrow$1 hash database model has the lowest F$_2$-scores. This is a result of overfitting of data. This method only returns the first camera make/model class where this set of JPEG quantization tables is seen. The tables are stored as a single hash and consequently it will not recognize a slightly modified set of JPEG quantization tables since this results in a completely different hash.

\begin{table}[h]
\begin{center}

\begin{tabular}{| c| c| c| c|}
\hline
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F$_2$-score}\\
\hline
Hash (1$\rightarrow$1) & 79 \% & 68 \% & 68 \%\\
Hash (1$\rightarrow N$) & 50 \% & 99 \% & 83 \%\\
Decision tree & 90 \% & 89 \% & 89 \% \\
\hline
\end{tabular}
\caption{Camera Make Identification}
\label{tab:fscore_make}
\end{center}

\end{table}

%1300 tables which have mutliple possibilities and thereby increasing the search space for fuurther research
\begin{table}[h]
\begin{center}
\begin{tabular}{| c| c| c| c|}
\hline
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F$_2$-score}\\
\hline
Hash (1$\rightarrow$1) & 54 \% & 39 \% & 37 \%\\
Hash (1$\rightarrow N$) & 50 \% & 98 \% & 83 \%\\
Decision tree & 78 \% & 82 \% & 80 \% \\
\hline
\end{tabular}
\caption{Camera Model Identification}
\label{tab:fscore_model}
\end{center}

\end{table}


\section{Discussion}

Since a decision tree uses a one-to-one mapping of a JPEG quantization table to a camera make/model, it will not perform perfectly on tables that occur at multiple camera make/models. The classifier makes a choice to which camera make/model this table is mapped. In contrast, the 
1$\rightarrow N$ hash database prediction model returns all possible camera makes/models. This method gains a high recall, but receives a low precision rate. Because all possibilities are returned, instead of only one camera make/model, the search space for these classes is significantly bigger than for decision tree learning. In order to decrease the search space for further processing with other camera identification methods, decision tree learning is preferred because it receives high scores for recall as well as precision.

Although the decision tree learning algorithm is prone to overfitting data, it will more accurately predict camera make and models for sets of JPEG quantization tables that differ slightly. The hash database models will not recognize the set and will return a random value in this implementation. The decision tree model, however, only uses a subset of the features found in the JPEG quantization tables and as a result can handle small differentiations better.

With respect to the creation of the prediction model, the hash database models are more easily trained. When a new set of JPEG quantization tables for a camera make/model occurs, it can be hashed and immediately stored in the database. In contrast, the decision tree model needs to be re-evaluated at the occurrence of a new set because this can result in a different decision tree.

It should be taken into account that the dataset only consists of original images. Image editing software such as Photoshop also use JPEG compression and will contain JPEG quantization tables correlated to Photoshop instead of their original camera make or model. For the decision tree learning algorithm, images that are edited with Photoshop will be predicted as belonging to the same source even though the original images are made with different camera makes and models. 
%----------------------------------------------------------------------------------------

